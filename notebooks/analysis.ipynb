{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Minari to generate episode data for conditional sequence modelling task episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Minimal version of S4D with extra options and features stripped out, for pedagogical purposes.\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from src.models.nn import DropoutNd\n",
    "\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = d_model\n",
    "        log_dt = torch.rand(H) * (\n",
    "            math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = d_model\n",
    "        self.n = d_state\n",
    "        self.d_output = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L) # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y, None # Return a dummy state to satisfy this repo's interface, but this can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from models.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n",
    "from models.s4.s4d import S4D\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, args.lr))\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "model = S4Model(\n",
    "    d_input=d_input,\n",
    "    d_output=d_output,\n",
    "    d_model=args.d_model,\n",
    "    n_layers=args.n_layers,\n",
    "    dropout=args.dropout,\n",
    "    prenorm=args.prenorm,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if args.resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "def setup_optimizer(model, lr, weight_decay, epochs):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically\n",
    "    require a smaller learning rate (typically 0.001), with no weight decay.\n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    # Print optimizer info\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer, scheduler = setup_optimizer(\n",
    "    model, lr=args.lr, weight_decay=args.weight_decay, epochs=args.epochs\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# Everything after this point is standard PyTorch training!\n",
    "###############################################################################\n",
    "\n",
    "# Training\n",
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )\n",
    "\n",
    "\n",
    "def eval(epoch, dataloader, checkpoint=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "            )\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if checkpoint:\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        return acc\n",
    "\n",
    "pbar = tqdm(range(start_epoch, args.epochs))\n",
    "for epoch in pbar:\n",
    "    if epoch == 0:\n",
    "        pbar.set_description('Epoch: %d' % (epoch))\n",
    "    else:\n",
    "        pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    train()\n",
    "    val_acc = eval(epoch, valloader, checkpoint=True)\n",
    "    eval(epoch, testloader)\n",
    "    scheduler.step()\n",
    "    # print(f\"Epoch {epoch} learning rate: {scheduler.get_last_lr()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Min GRU and Min LSTM Implementations\n",
    "\n",
    "class MinGRU(nn.Module):\n",
    "    def __init__(self, d_model, d_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from minGRU_pytorch import minGRU\n",
    "\n",
    "min_gru = minGRU(512)\n",
    "\n",
    "x = torch.randn(2, 1024, 512)\n",
    "\n",
    "out = min_gru(x)\n",
    "\n",
    "assert x.shape == out.shapt torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from minGRU_pytorch import minGRU\n",
    "\n",
    "min_gru = minGRU(dim = 512, expansion_factor = 1.5)\n",
    "\n",
    "x = torch.randn(1, 2048, 512)\n",
    "\n",
    "# parallel\n",
    "\n",
    "parallel_out = min_gru(x)[:, -1:]\n",
    "\n",
    "# sequential\n",
    "\n",
    "prev_hidden = None\n",
    "for token in x.unbind(dim = 1):\n",
    "    sequential_out, prev_hidden = min_gru(token[:, None, :], prev_hidden, return_next_prev_hidden = True)\n",
    "\n",
    "assert torch.allclose(parallel_out, sequential_out, atol = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SequentialReplayBuffer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 512,\n",
    "        buffer_size: int = 100000,\n",
    "        sequence_length: int = 50,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = buffer_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize MinGRU encoder\n",
    "        self.encoder = MinGRU(\n",
    "            input_dim=state_dim + action_dim,  # Concatenated state-action pairs\n",
    "            hidden_dim=hidden_dim\n",
    "        ).to(device)\n",
    "\n",
    "        # Storage\n",
    "        self.states = deque(maxlen=buffer_size)\n",
    "        self.actions = deque(maxlen=buffer_size)\n",
    "        self.rewards = deque(maxlen=buffer_size)\n",
    "        self.next_states = deque(maxlen=buffer_size)\n",
    "        self.dones = deque(maxlen=buffer_size)\n",
    "        \n",
    "        self.episode_boundaries = []  # Track episode start indices\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "        if done:\n",
    "            self.episode_boundaries.append(len(self.states) - 1)\n",
    "\n",
    "    def encode_sequence(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode a sequence of state-action pairs using MinGRU\"\"\"\n",
    "        # Concatenate states and actions\n",
    "        sequence = torch.cat([states, actions], dim=-1)\n",
    "        # Encode sequence\n",
    "        encoded = self.encoder(sequence)\n",
    "        return encoded\n",
    "\n",
    "    def sample_sequence(self, batch_size: int) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Sample a batch of sequences\"\"\"\n",
    "        # Randomly select sequence start points\n",
    "        max_start_idx = len(self.states) - self.sequence_length\n",
    "        start_indices = np.random.randint(0, max_start_idx, size=batch_size)\n",
    "        \n",
    "        # Gather sequences\n",
    "        state_sequences = []\n",
    "        action_sequences = []\n",
    "        reward_sequences = []\n",
    "        next_state_sequences = []\n",
    "        done_sequences = []\n",
    "        \n",
    "        for start_idx in start_indices:\n",
    "            # Check if sequence crosses episode boundary\n",
    "            end_idx = start_idx + self.sequence_length\n",
    "            valid = all(boundary not in range(start_idx, end_idx) \n",
    "                       for boundary in self.episode_boundaries)\n",
    "            \n",
    "            if not valid:\n",
    "                # Resample if sequence crosses episode boundary\n",
    "                continue\n",
    "                \n",
    "            states = torch.tensor([self.states[i] for i in range(start_idx, end_idx)])\n",
    "            actions = torch.tensor([self.actions[i] for i in range(start_idx, end_idx)])\n",
    "            rewards = torch.tensor([self.rewards[i] for i in range(start_idx, end_idx)])\n",
    "            next_states = torch.tensor([self.next_states[i] for i in range(start_idx, end_idx)])\n",
    "            dones = torch.tensor([self.dones[i] for i in range(start_idx, end_idx)])\n",
    "            \n",
    "            state_sequences.append(states)\n",
    "            action_sequences.append(actions)\n",
    "            reward_sequences.append(rewards)\n",
    "            next_state_sequences.append(next_states)\n",
    "            done_sequences.append(dones)\n",
    "\n",
    "        # Stack sequences\n",
    "        states = torch.stack(state_sequences).to(self.device)\n",
    "        actions = torch.stack(action_sequences).to(self.device)\n",
    "        rewards = torch.stack(reward_sequences).to(self.device)\n",
    "        next_states = torch.stack(next_state_sequences).to(self.device)\n",
    "        dones = torch.stack(done_sequences).to(self.device)\n",
    "\n",
    "        # Encode sequences\n",
    "        encoded_states = self.encode_sequence(states, actions)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones, encoded_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SequentialTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        agent,\n",
    "        replay_buffer,\n",
    "        batch_size: int = 32,\n",
    "        updates_per_step: int = 1,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.updates_per_step = updates_per_step\n",
    "        self.device = device\n",
    "\n",
    "    def train_episode(self, max_steps: int = 1000):\n",
    "        state = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = self.agent.select_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            if len(self.replay_buffer.states) > self.batch_size:\n",
    "                for _ in range(self.updates_per_step):\n",
    "                    sequences = self.replay_buffer.sample_sequence(self.batch_size)\n",
    "                    self.agent.update(sequences)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        return episode_reward\n",
    "\n",
    "    def train(self, num_episodes: int = 1000):\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            episode_reward = self.train_episode()\n",
    "            rewards.append(episode_reward)\n",
    "            \n",
    "            # Log progress\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                avg_reward = sum(rewards[-10:]) / 10\n",
    "                print(f\"Episode {episode+1}: Average Reward = {avg_reward:.2f}\")\n",
    "                \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinGRUForRL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Additional layers for RL-specific processing\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        hidden: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Process sequence through GRU\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Apply self-attention over the sequence\n",
    "        attn_output, _ = self.attention(output, output, output)\n",
    "        \n",
    "        # Residual connection and normalization\n",
    "        output = self.norm(output + attn_output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Dict, Optional\n",
    "\n",
    "class OfflineRLAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        sequence_encoder: nn.Module,\n",
    "        hidden_dim: int = 512,\n",
    "        cql_alpha: float = 1.0,\n",
    "        discount: float = 0.99,\n",
    "        tau: float = 0.005,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.action_dim = action_dim\n",
    "        self.cql_alpha = cql_alpha\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "\n",
    "        # Sequence encoder (MinGRU)\n",
    "        self.sequence_encoder = sequence_encoder\n",
    "\n",
    "        # Q-networks\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Target networks\n",
    "        self.q1_target = nn.Sequential(*[copy.deepcopy(layer) for layer in self.q1])\n",
    "        self.q2_target = nn.Sequential(*[copy.deepcopy(layer) for layer in self.q2])\n",
    "\n",
    "        # Policy network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim * 2)  # Mean and log_std\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        self.to(device)\n",
    "\n",
    "    def encode_sequence(\n",
    "        self, \n",
    "        states: torch.Tensor, \n",
    "        actions: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode state-action sequence using MinGRU\"\"\"\n",
    "        sequence = torch.cat([states, actions], dim=-1)\n",
    "        encoded, _ = self.sequence_encoder(sequence)\n",
    "        return encoded[:, -1]  # Return last hidden state\n",
    "\n",
    "    def get_action(\n",
    "        self, \n",
    "        encoded_state: torch.Tensor, \n",
    "        deterministic: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        mean, log_std = self.policy(encoded_state).chunk(2, dim=-1)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "\n",
    "        if deterministic:\n",
    "            return mean\n",
    "\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        action = normal.rsample()\n",
    "        return torch.tanh(action)\n",
    "\n",
    "    def compute_q_values(\n",
    "        self, \n",
    "        encoded_states: torch.Tensor, \n",
    "        actions: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Compute Q-values from both networks\"\"\"\n",
    "        q_input = torch.cat([encoded_states, actions], dim=-1)\n",
    "        return self.q1(q_input), self.q2(q_input)\n",
    "\n",
    "    def update(self, batch: Tuple[torch.Tensor, ...]) -> Dict[str, float]:\n",
    "        \"\"\"Update agent using CQL\"\"\"\n",
    "        states, actions, rewards, next_states, dones, encoded_states = batch\n",
    "        batch_size = states.shape[0]\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_encoded = self.encode_sequence(next_states, actions)\n",
    "            next_actions = self.get_action(next_encoded)\n",
    "            target_q1, target_q2 = self.compute_target_q_values(next_encoded, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2)\n",
    "            target_q = rewards + (1 - dones) * self.discount * target_q\n",
    "\n",
    "        # Current Q-values\n",
    "        current_q1, current_q2 = self.compute_q_values(encoded_states, actions)\n",
    "\n",
    "        # CQL loss\n",
    "        random_actions = torch.FloatTensor(batch_size, self.action_dim).uniform_(-1, 1).to(self.device)\n",
    "        random_q1, random_q2 = self.compute_q_values(encoded_states, random_actions)\n",
    "        \n",
    "        cql_loss_q1 = torch.logsumexp(random_q1, dim=0) - current_q1.mean()\n",
    "        cql_loss_q2 = torch.logsumexp(random_q2, dim=0) - current_q2.mean()\n",
    "        cql_loss = self.cql_alpha * (cql_loss_q1 + cql_loss_q2)\n",
    "\n",
    "        # Standard Q-learning loss\n",
    "        q_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = q_loss + cql_loss\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        self._soft_update_target()\n",
    "\n",
    "        return {\n",
    "            'q_loss': q_loss.item(),\n",
    "            'cql_loss': cql_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "    def _soft_update_target(self):\n",
    "        \"\"\"Soft update target networks\"\"\"\n",
    "        for param, target_param in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.q2.parameters(), self.q2_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save agent state\"\"\"\n",
    "        torch.save({\n",
    "            'sequence_encoder': self.sequence_encoder.state_dict(),\n",
    "            'q1': self.q1.state_dict(),\n",
    "            'q2': self.q2.state_dict(),\n",
    "            'policy': self.policy.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load agent state\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.sequence_encoder.load_state_dict(checkpoint['sequence_encoder'])\n",
    "        self.q1.load_state_dict(checkpoint['q1'])\n",
    "        self.q2.load_state_dict(checkpoint['q2'])\n",
    "        self.policy.load_state_dict(checkpoint['policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "# from src.replay.sequential_buffer import SequentialReplayBuffer\n",
    "# from src.training.sequential_trainer import SequentialTrainer\n",
    "# from src.models.mingru_rl import MinGRUForRL\n",
    "\n",
    "def main():\n",
    "    # Environment setup\n",
    "    env = gym.make('HalfCheetah-v2')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    # Create replay buffer\n",
    "    buffer = SequentialReplayBuffer(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=512,\n",
    "        sequence_length=50\n",
    "    )\n",
    "    \n",
    "    # Create agent (you'll need to implement this based on your RL algorithm)\n",
    "    agent = YourRLAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        sequence_encoder=MinGRUForRL(\n",
    "            input_dim=state_dim + action_dim,\n",
    "            hidden_dim=512\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SequentialTrainer(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        replay_buffer=buffer,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    rewards = trainer.train(num_episodes=1000)\n",
    "    \n",
    "    # Save results\n",
    "    torch.save({\n",
    "        'rewards': rewards,\n",
    "        'agent_state': agent.state_dict(),\n",
    "        'buffer_encoder': buffer.encoder.state_dict()\n",
    "    }, 'results/sequential_rl_experiment.pt')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
